name: Data Pipeline Automation

on:
  schedule:
    # Weekly updates for interest rates (Mondays at 2 AM UTC)
    - cron: '0 2 * * 1'
    # Monthly updates for real estate metrics (1st of month at 3 AM UTC)  
    - cron: '0 3 1 * *'
    # Quarterly updates for lending requirements (1st day of quarter at 4 AM UTC)
    - cron: '0 4 1 1,4,7,10 *'
  
  workflow_dispatch:
    inputs:
      update_type:
        description: 'Type of update to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - interest_rates
        - real_estate
        - lending_requirements
        - single_parameter
      parameter_name:
        description: 'Specific parameter (if single_parameter selected)'
        required: false
        type: string

jobs:
  data-update:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Setup data directories
      run: |
        mkdir -p data/databases/backups
        mkdir -p logs
    
    - name: Create backup of current databases
      run: |
        python -c "
        import shutil
        import datetime
        from pathlib import Path
        
        backup_dir = Path('data/databases/backups')
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        
        databases = ['market_data.db', 'property_data.db', 'economic_data.db', 'forecast_cache.db']
        
        for db in databases:
            src = Path(f'data/databases/{db}')
            if src.exists():
                dst = backup_dir / f'{db.replace(\".db\", \"\")}_backup_{timestamp}.db'
                shutil.copy2(src, dst)
                print(f'Backed up {db} to {dst}')
        "
    
    - name: Determine update type
      id: update_type
      run: |
        if [ "${{ github.event_name }}" = "schedule" ]; then
          case "${{ github.event.schedule }}" in
            "0 2 * * 1")
              echo "update_type=interest_rates" >> $GITHUB_OUTPUT
              ;;
            "0 3 1 * *")
              echo "update_type=real_estate" >> $GITHUB_OUTPUT
              ;;
            "0 4 1 1,4,7,10 *")
              echo "update_type=lending_requirements" >> $GITHUB_OUTPUT
              ;;
          esac
        else
          echo "update_type=${{ github.event.inputs.update_type }}" >> $GITHUB_OUTPUT
        fi
    
    - name: Run data updates
      env:
        FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        UPDATE_TYPE: ${{ steps.update_type.outputs.update_type }}
        PARAMETER_NAME: ${{ github.event.inputs.parameter_name }}
      run: |
        python scripts/manage_scheduler.py --action=update --type=${UPDATE_TYPE} --parameter=${PARAMETER_NAME}
    
    - name: Validate updated data
      run: |
        echo "Validating data integrity after updates..."
        python -m pytest tests/integration/test_production_data_validation.py -v
    
    - name: Run end-to-end workflow test
      run: |
        echo "Testing complete DCF workflow with updated data..."
        python demo_end_to_end_workflow.py
    
    - name: Generate data freshness report
      run: |
        python -c "
        from data.scheduler.data_scheduler import create_scheduler
        import json
        
        scheduler = create_scheduler()
        freshness_report = scheduler.get_data_freshness_report()
        status = scheduler.get_scheduler_status()
        
        # Save reports
        with open('data_freshness_report.json', 'w') as f:
            json.dump({
                'freshness_report': [
                    {
                        'parameter_name': r.parameter_name,
                        'geographic_code': r.geographic_code,
                        'last_data_date': r.last_data_date.isoformat(),
                        'days_since_update': r.days_since_update,
                        'is_stale': r.is_stale,
                        'recommended_action': r.recommended_action
                    } for r in freshness_report
                ],
                'scheduler_status': {
                    'total_scheduled_updates': status['total_scheduled_updates'],
                    'enabled_updates': status['enabled_updates'],
                    'stale_parameters_count': status['stale_parameters_count']
                }
            }, f, indent=2)
        
        print(f'Data freshness report: {status[\"stale_parameters_count\"]} stale parameters')
        "
    
    - name: Upload data reports
      uses: actions/upload-artifact@v3
      with:
        name: data-pipeline-reports
        path: |
          data_freshness_report.json
          logs/pro_forma_analytics.log
    
    - name: Create issue if data is stale
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          let reportData = {};
          
          try {
            reportData = JSON.parse(fs.readFileSync('data_freshness_report.json', 'utf8'));
          } catch (e) {
            reportData = { scheduler_status: { stale_parameters_count: 'unknown' } };
          }
          
          const staleCount = reportData.scheduler_status?.stale_parameters_count || 'unknown';
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Data Pipeline Alert: ${staleCount} stale parameters detected`,
            body: `
          ## Data Pipeline Failure Alert
          
          The automated data pipeline has detected issues during the scheduled update.
          
          **Stale Parameters:** ${staleCount}
          **Update Type:** ${{ steps.update_type.outputs.update_type }}
          **Workflow Run:** [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
          
          Please investigate the data pipeline and ensure all parameters are being updated correctly.
          
          **Recommended Actions:**
          1. Check the workflow logs for specific errors
          2. Verify API keys and data source availability  
          3. Run manual data updates if needed
          4. Review data freshness report in workflow artifacts
          
          This issue was automatically created by the data pipeline automation.
            `,
            labels: ['data-pipeline', 'automated-alert', 'high-priority']
          });

  health-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Perform data health check
      run: |
        python -c "
        from data.scheduler.data_scheduler import create_scheduler
        import sys
        
        scheduler = create_scheduler()
        freshness_report = scheduler.get_data_freshness_report()
        stale_parameters = [r for r in freshness_report if r.is_stale]
        
        print(f'Health Check Results:')
        print(f'Total parameters: {len(freshness_report)}')
        print(f'Stale parameters: {len(stale_parameters)}')
        
        if stale_parameters:
            print('\nStale Parameters:')
            for param in stale_parameters:
                print(f'  - {param.parameter_name} ({param.geographic_code}): {param.days_since_update} days old')
            
            # Exit with error code if critical parameters are stale
            critical_params = ['treasury_10y', 'commercial_mortgage_rate', 'cap_rate']
            stale_critical = [p for p in stale_parameters if p.parameter_name in critical_params]
            
            if stale_critical:
                print(f'\nCRITICAL: {len(stale_critical)} critical parameters are stale!')
                sys.exit(1)
        else:
            print('All parameters are fresh!')
        "
    
    - name: Create health check summary
      run: |
        echo "## Data Pipeline Health Check Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Check Date:** $(date -u)" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "See workflow logs for detailed parameter freshness information." >> $GITHUB_STEP_SUMMARY